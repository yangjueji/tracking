\documentclass{acm_proc_article-sp}

\usepackage{amsmath,amssymb}
\usepackage{latexsym}
\usepackage{indentfirst}
\usepackage{blindtext}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{url}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\minbelow}{min}

\begin{document}

\title{Object Tracking}
\subtitle{A Quantitative Comparison}
\author{Chuhang Zou, Zheng Yan, Yi Shi, Qing Ren, Caihua Shan, Jueji Yang}
\maketitle

\begin{abstract}
Object tracking is a such import task in computer vision that many kinds of methods are developed to solve this problem. Most of these methods have released their codes and datasets on the Internet. It is meaningful to evaluate these methods with unified criteria. We will briefly introduce these algorithms and then pick some datasets from Internet to test all the algorithms on some criterion to see their performance and make an advise about what kind of image sequence each algorithm is better for.
\end{abstract}

\section{Introduction}
Object tracking is one of the core problems in a wide range of applications in computer vision, such as surveillance, human computer interaction, augmented reality, and medical imaging.
In most areas of computer vision like scene understanding and action recognition, object tracking is regarded as an essential component.
Object tracking problem is to estimate the position of target in video sequence given the position of the target in the first frame.
Lots of algorithms and studies have been established to solve object tracking problem in the recent years.
But most algorithms perform bad when occlusion or out-of-view appears in the sequence.
Therefore, we evaluate the performance over the data sequence separately and discuss the bad cases for each algorithm.

The input data of object tracking problem can be image sequence or videos.
The predict position of the target in a frame is rectangle which can be 2 diagonal angles or 1 angle with width and height.
It needs lots of human effort to sample images using camera and label the position of the target by our own.
And the content of the images are not rich, most of which will be our faces.
Thus, we collect our data from the Internet\cite{dataset}.
These datas contain out-of-view, occlusion, illumination variation, scale variation, deformation, motion blur, fast motion, background clutters and other factors that will cause large tracking inaccuracy which is rich enough to test these state-of-art algorithms.

There are large mount of algorithms released to the public including Struck\cite{struck}, deep learning track\cite{dlt}, DCMT\cite{dcmt}, ASLA\cite{asla}, TLD\cite{tld}, CXT\cite{cxt} and other state-of-art methods that perform quite well in their papers. 
We find these methods have mainly two problems: 
1). Overfitting on the initial frame with initial position, which will cause target missing when occlusion, out-of-plane rotation happen, but perform better in fast motion and out-of-view situations;
2). Online training over the sequence and overfitting the following tracked images will cause target shift when out-of-view happened.
To test the algorithms on these two problems, we pick out some notable image sequences, and discuss the performance of each algorithm to see whether the two problems happened on them.

\section{Related Work}


\section{Evaluated Algorithms}
In this report, we investigate 6 visual tracking works and search for an appropriate evaluation criteria for those work. The algorithm details of these 6 tracking methods are listed as follow:

\subsection{Struck}
Struck\cite{struck} is different from other tracking-by-detection algorithms. Struck learns a function that directly estimates the object transformation between frames. Discriminant function $F:\mathcal{X} \times \mathcal{Y} \to \mathcal{R}$ with $X$ the sequence of images and $Y$ the possible bounding boxes transformations is used to predict:
\[
y_t = f(x_t^{P_{t-1}}) = \argmax_{y\in \mathcal{Y}}F(x_t^{P_{t-1}},y)
\]
where $t$ is time. $x_t^{P_{t-1}}$is the image with estimated bounding box at time $t-1$ and image pixels at time $t$. $F$ measures the compatibility between $(x,y)$ pairs, when $x$ and $y$ are well matched, $F$ gives a high score:
\[
F(x,y)=\langle w, \Phi(x,y) \rangle
\]
The parameter $w$ can be optimized by:
\begin{align}
\minbelow_w	&\frac{1}{2}||w||^2 + C \sum_{i=1}^n\xi_i\nonumber\\
	s.t.	&\forall i: \xi_i \ge 0\nonumber\\
			&\forall i, \forall y\not= y_i : \langle w, \delta\Phi_i(y)\rangle \ge \Delta(y_i,y)-\xi_i
\end{align}
$\delta\Phi_i(y)=\Phi(x_i, y_i) - \Phi(x_i, y)$. This optimization aims to ensure that the value of $F(x_i, y_i)$ is greater than $F(x_i, y)$ for $y\not= y_i$. $1-\Delta(y,\bar{y})$ is the bounding box overlap between $y$ and $\bar{y}$.

The paper optimize the dual form of the optimization which will not be listed here in detail. $\Phi(x,y)$ only appears in inner products in the dual form of this optimization. So a joint kernel function $k(x,y,\bar{x},\bar{y}) = \langle \Phi(x,y), \Phi(\bar{x},\bar{y})\rangle$ is used which is the kernel between features(Haar, Histogram) of cropped boxes of images. And SMO-style step is also used to optimize the dual secondary convex optimization.

\subsection{The Deep Learning Tracker}
According to \cite{dlt}, they propose a novel deep learning tracker (DLT) for robust visual tracking.

During the offline training stage, they train an large stacked denoising autoencoder (SDAE) containing five small denoising autoencoders (DAEs) with the Tiny Images dataset \cite{tiny} as auxiliary image data to learn generic natural image features.
We show the architecture of DAE and the whole structure of the SDAE in Fig.~\ref{fig:dlt}(a) and Fig.~\ref{fig:dlt}(b) respectively.

During the online tracking process, an additional classification layer is added to the encoder part of the trained SDAE to result in a classification neural network.
The object to track is specified by the location of its bounding box in the first frame as a positive example.
Some negative examples are collected from the background at a short distance from the object in order to fine-tune the classification layer.
The whole network can be tuned in many times during the entire tracking process.
The overall network architecture is shown in Fig.~\ref{fig:dlt}(c).

\begin{center}
    \begin{figure}[hbt]
      \includegraphics[width=0.5\textwidth]{dlt.png}
      \caption{Some key components of the network architecture: (a) denoising autoencoder; (b) stacked denoising autoencoder; (c) network for online tracking.}
      \label{fig:dlt}
    \end{figure}
\end{center}

\subsection{Context Tracker}
    CXT(Context Tracker) uses additional context information to build a strong model. It uses two different terms: 1) Distracters are regions that have similar appearance as the target, 2) Supporters are local key-points around the object having motion correlation with the target in a short time span. The goal of this algorithm is to find all possible regions which look similar to the target to prevent drift, and to look for useful information around the target to have strong verification.
    
    The paper uses P-N Tracker as the basic target tracker with several extensions. It extends the randomized ferns to accept multiple objects, applies 6bitBP which helps to boost up the speed of the detector and uses online template-based object model by constructing it in binary search tree using k-means.
    \newline
    As for distracters, a testing sample confidence score is computed using Normalized Cross-Correlation(NCC) between it and the closest image patch in the object model. It chooses the best candidate as the tracking result and the remaining regions trigger new distracter trackers.
    \newline
    Assuming that there are the valid target at frame t, the supporters are extracted around the location of that target with a radius R.
    \newline
    In unconstrained environments, the target may leave the FoV, or be completely occluded by other objects. The common trackers will simply switch to another region satisfying the threshold. Here, this tracker automatically exploits all the distracters and pays attention to them by tracking them simultaneously. Also, this tracker discovers a set of supporters to robustly identify the target among other similar regions.

\subsection{ASLA}

\label{sec:asla_section}

Visual Tracking via Adaptive Structural Local Sparse Appearance Model (ASLA) is one of Posters in CVPR2012 and performs favorably against several state-of-the-art methods on benchmark challenge.

ASLA is an efficient methods combined with structural local sparse model and adaptive template update strategy. It samples overlapped local image patches within the target region, whose sparse coding obtained by a novel alignment-pooling method contains both spatial and partial information of the target object. This representation guarantees more accurately location and less occlusion drift. In addition, the online adaptive template strategy based on both incremental subspace learning and sparse representation is employed in coding, far more improving the performance compared by methods based on static local or holistic sparse dictionary when there is similar object in the scenes.

The proposed algorithm is implemented in MATLAB and runs at 1.5 frames per second on a Pentium 2.7 GHz Dual Core PC with 2GB memory.

The $l_1$ minimization problem is solved with the SPAMS package and the regularization constant $\lambda$ is set to 0.01 in all experiments. For each sequence, the location of the target object is manually labeled in the first frame. We resize the target image patch to $32\times 32$ pixels and extract overlapped $16\times16$ local patches within the target region with 8 pixels as step length. As for the template update, 8 eigenvectors are used to carry out incremental subspace learning method in all experiments every 5 frames.

\subsection{TLD}
Tracking is a fundamental problem in computer vision, sometimes it is desirable to track an object with little prior knowledge. Almost all the methods by Tracking-by-detection, a extremely useful approach, have one thing in common, that is a sparse sampling strategy. Several samples, near the target, will be collected in every frame. For each sample, it typically characterizes a subwindow the same size as the target and most of the samples have a huge amount of overlap, causing a lot of redundancy. In order to overcome this weakness, we address a new theory. We show that the process of taking subwindows of an image indce circulant structure. Then establishing links to Fourier analysis that allows the use of the Fast Fourier Transform to quickly incorporate information from all subwindows, without iterating over them. Classification on non-linear feature spaces with the Kernel Trick can be done as efficiently as in the original image space. All running in $O(n^2 logn)$ for $n * n$ images.

\subsection{Discrete-Continuous Optimization for Multi-Target Tracking}
Recent approaches for multi-target tracking often use discrete optimization. However, this has the disadvantage that trajectories need to be pre-computed or represented discretely, thus limiting accuracy. In this paper, the author instead formulate multi-target tracking as a discrete-continuous optimization problem that handles each aspect in its natural domain and allows leveraging powerful methods for multi-model fitting. Data association is performed using discrete optimization with label costs, yielding near optimality. Trajectory estimation is posed as a continuous fitting problem with a simple closed-form solution, which is used in turn to update the label costs.

Targets are separated from the background in a preprocessing step and form a set of target hypotheses, which are then used to infer the targets¡¯ trajectories. Then algorithm then run a sliding window detector, based on SVM classification of histograms of oriented gradients (HOG) and relative optical flow (HOF). The detector yields a set of target hypotheses D.

Given the set of target hypotheses D, the goal is to identify a set of target trajectories $\tau = {\tau_1,\ldots,\tau_N}$. This implies that it also need to search for a data association $f$, which for each detection $d \in D$ assigns a label $f_f \in L = {1,\ldots,N}\bigcup \emptyset$. Thereby a detection is either identified as belonging to one of the trajectories or, using the additional outlier label $\emptyset$, identified as a false alarm.
The eventual aim is to perform multi-target tracking by minimizing a joint energy $E(\tau , f)$ w.r.t. the trajectories $\tau$ and the data association $f$. 


\section{Evaluation Criterion}

In this report, we use success plot for quantitative analysis.
In addition, we evaluate speed of each algorithm by average running time in fps.

\textbf{Success plot.}
One widely used evaluation metric on tracking precision is the bounding box overlap. 
Given the tracked bounding box $r_t$ and the ground truth bounding box $r_a$, the overlap score is defined as $S = \frac{|r_t \bigcap r_a|}{|r_t \bigcup r_a|}$.
And then calculate the percentage of frames that has a score bigger than overlap threshold showed in Fig.~\ref{fig:soccer}.
The success plot shows the ratios of successful frames at the thresholds varied from 0 to 1. 

\textbf{Average running time.}
Average running time is a metric which can judge whether the algorithm is sufficient for real-time applications.
We represent average running time in fps to give readers a more direct sense.


\section{Experiments}

We choose 10 image sequences from website \cite{dataset} to cover all situations.
For each tracker, we fine-tuned parameters in the source code by our hand for each sequences.

\subsection{Sequences Analysis}

The performance in each sequence for all the trackers is summarized by the success plots as shown in Fig~\ref{fig:soccer} to Fig~\ref{fig:woman}.

\begin{figure}[hbt]
    \includegraphics[width=200pt]{soccer.jpg}
    \caption{success rate score plot of soccer sequences}
    \label{fig:soccer}
\end{figure}

\begin{figure}[hbt]
    \includegraphics[width=200pt]{subway.jpg}
    \caption{success rate score plot of subway sequences}
    \label{fig:subway}
\end{figure}

\begin{figure}[hbt]
    \includegraphics[width=200pt]{suv.jpg}
    \caption{success rate score plot of suv sequences}
    \label{fig:suv}
\end{figure}

\begin{figure}[hbt]
    \includegraphics[width=200pt]{sylvester.jpg}
    \caption{success rate score plot of sylvester sequences}
    \label{fig:sylvester}
\end{figure}

\begin{figure}[hbt]
    \includegraphics[width=200pt]{tiger1.jpg}
    \caption{success rate score plot of tiger1 sequences}
    \label{fig:tiger1}
\end{figure}

\begin{figure}[hbt]
    \includegraphics[width=200pt]{tiger2.jpg}
    \caption{success rate score plot of tiger2 sequences}
    \label{fig:tiger2}
\end{figure}

\begin{figure}[hbt]
    \includegraphics[width=200pt]{trellis.jpg}
    \caption{success rate score plot of trellis sequences}
    \label{fig:trellis}
\end{figure}

\begin{figure}[hbt]
    \includegraphics[width=200pt]{walking.jpg}
    \caption{success rate score plot of walking sequences}
    \label{fig:walking}
\end{figure}

\begin{figure}[hbt]
    \includegraphics[width=200pt]{walking2.jpg}
    \caption{success rate score plot of walking2 sequences}
    \label{fig:walking2}
\end{figure}

\begin{figure}[hbt]
  \includegraphics[width=200pt]{woman.jpg}
  \caption{success rate score plot of woman sequences}
  \label{fig:woman}
\end{figure}

Struck detects 9 frames per second with budget size of 100 and Haar feature.
In these ten image sequence, truck performs not good in Soccer and SUV.
Soccer image has many pieces of ribbons which has stronger effect on Haar feature than human faces.
The big variance of background near the target causes bad performance.
In SUV image, the SUV is occluded by trees and traffic lights, histogram of color feature is litter better than the Haar feature.
SUV's visual feature is also like the background road which is a reason for detected position stuck in the road.

DLT detects 4~5 frames per second. In the tiger1 and tiger2 sequence, the target is a fast moving tool with motion blur. DLT with a higher value of x and y translation can track tiger1 successfully, while lost track of tiger2 in about frame 160. In the woman sequence, the woman is severely occluded several times by the parked cars. DLT tracker fails when the woman walks close to the car at about frame 130.

In the Woman, Walking, Walking2 and Subway sequence, the target is obstructed by moving or feature-similar objects, ASLA performs bad in this sequence. In the Sylvester sequence，the target is moving with illumination varying, ASLA does well in this sequence. In other sequence, it got lost in some frames but follows almost frames overall. From another respective,  though the tracker follows the target, the areas it predict has deviation compared with ground truth.

\begin{table}
	\centering
	\begin{tabular}{|c|c|l|} \hline
		Method & FPS\\ \hline
		Struck & 9.3\\ \hline
		DLT & 17.0\\ \hline
		CXT & 15.3\\ \hline
		ASLA & 8.5\\ \hline
		DCMT & ?.?\\ \hline
		TLD & ?.?\\ \hline
	\end{tabular}
	\caption{Comparison of average running time on woman video sequences (in fps).}
	\label{table:time}
\end{table}

We also list the average running time of 10 sequence in detail in Table ~\ref{table:time}. Thanks to advances of the GPU technology, DLT can achieve an average frame rate of 17 fps\footnote{Running on a PC with Nvidia GTX 760} which is sufficient for many real-time applications.


\section{Conclusions}
In this paper, we run massive amount of experiments to evaluate the performance of 6 selected tracking algorithms.
We use different image sequences to evaluate algorithms in different situations.
Each algorithm has its own better situation and worse situation.
Struck is good for slow moving objects which do not leave the view. And the target should better not have background clutters which can make the image feature significantly different.
Such as DLT is good at handling occlusion, while no so good at handling abrupt illumination changes.


\begin{thebibliography}{99}
\bibitem{struck}
S. Hare, A. Saffari, and P. H. S. Torr. Struck: Structured Output Tracking with Kernels. In ICCV, 2011.

\bibitem{shi1}
A. Adam, E. Rivlin, and I. Shimshoni. Robust fragments-based tracking using the integral histogram. In CVPR, 2006.

\bibitem{shi2}
B. Babenko, M.-H. Yang, and S. Belongie. Visual tracking with online multiple instance learning. In CVPR, 2009.

\bibitem{shi3}
J. Kwon and K. M. Lee. Visual tracking decomposition. In CVPR, 2010.

\bibitem{shi4}
D. Ross, J. Lim, R.-S. Lin, and M.-H. Yang. Incremental learning for robust visual tracking. IJCV, 77(1): 125141, 2008.

\bibitem{shi5}
J. Santner, C. Leistner, A. Saffari, T. Pock, and H. Bischof. Prost: Parallel robust online simple tracking. In CVPR, 2010.

\bibitem{shi6}
X. Mei and H. Ling. Robust visual tracking using L1 minimization. In ICCV, 2009.

\bibitem{shi7}
Z. Kalal, J. Matas, and K. Mikolajczyk. P-N learning: Bootstrapping binary classifiers by structural constraints. In CVPR, 2010.

\bibitem{dlt}
Wang, Naiyan, and Dit-Yan Yeung. Learning a Deep Compact Image Representation for Visual Tracking. Advances in Neural Information Processing Systems. 2013.

\bibitem{tiny}
Torralba, Antonio, Robert Fergus, and William T. Freeman. 80 million tiny images: A large data set for nonparametric object and scene recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on 30.11 (2008): 1958-1970.

\bibitem{benchmark}
Y.Wu, J. Lim, M.H. Y. Online Object Tracking: A Benchmark. In CVPR, 2013

\bibitem{asla}
X.Jia, H.Lu. M.H Yang, Visual Tracking via Adaptive Structural Local Sparse Appearance Model, In CVPR, 2012

\bibitem{tld}
Z. Kalal, J. Matas, and K. Mikolajczyk. P-N Learning: Bootstrapping Binary Classifiers by Structural Constraints. In CVPR, 2010.

\bibitem{cxt}
T. B. Dinh, N. Vo, and G. Medioni. Context Tracker: Exploring supporters and distracters in unconstrained environments. In CVPR, 2011.

\bibitem{dcmt}
A.Andriyenko, K.Schindler, S.Roth. Discrete-Continuous Optimization for Multi-Target Tracking. CVPR 2012.


\bibitem{dataset}
\url{https://sites.google.com/site/trackerbenchmark/benchmarks/v10}

\end{thebibliography}

\end{document}
